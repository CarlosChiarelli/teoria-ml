{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "This is a pure numpy implementation of word generation using an RNN. \n",
    "Original code: https://github.com/llSourcell/recurrent_neural_network\n",
    "\n",
    "\n",
    "## Our steps\n",
    "\n",
    "- Initialize weights randomly\n",
    "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
    "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
    "- Measure error (the distance between the previous probability and the target char)\n",
    "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
    "- update all parameters in the direction via gradients that help to minimise the loss\n",
    "- Repeat! Until our loss is small AF\n",
    "\n",
    "## What are some use cases?\n",
    "\n",
    "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
    "- Sequential data generation (music, video, audio, etc.)\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
    "\n",
    "## What's next? \n",
    "\n",
    "1 LSTM Networks\n",
    "2 Bidirectional networks\n",
    "3 recursive networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * encode char into vectors\n",
    "* Define the Recurrent Network\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Define a function to create sentences from the model\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradient and update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like.\n",
    "\n",
    "### So First let's calculate the *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137628 chars 80 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"data has \" + str(data_size) + \" chars \" + str(vocab_size) + \" unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "\n",
    "Neural networks operate on vectors (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
    "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### Then we create 2 dictionary to encode and decode a char to an int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 0, '8': 1, 'Q': 2, 's': 3, 'M': 4, '@': 5, 'G': 6, '!': 7, 'D': 8, 'S': 9, 'W': 10, 'N': 11, 'ç': 12, 'i': 13, \"'\": 14, 'H': 15, '$': 16, 'I': 17, 'K': 18, 'h': 19, 'b': 20, 'w': 21, ':': 22, '0': 23, '%': 24, 'X': 25, 'x': 26, '/': 27, '1': 28, 'J': 29, '2': 30, '7': 31, '9': 32, 'o': 33, 'R': 34, 'P': 35, ')': 36, 'a': 37, 'c': 38, 'u': 39, 'r': 40, 'd': 41, ' ': 42, 'm': 43, '\"': 44, '4': 45, '3': 46, ',': 47, 'C': 48, 'n': 49, 'f': 50, 'p': 51, '\\n': 52, ';': 53, 'Y': 54, 'v': 55, 'A': 56, 'F': 57, 'g': 58, 't': 59, '.': 60, 'V': 61, '5': 62, 'q': 63, '-': 64, 'E': 65, 'U': 66, '6': 67, 'y': 68, 'T': 69, '(': 70, 'z': 71, 'O': 72, 'l': 73, '?': 74, 'e': 75, '*': 76, 'L': 77, 'j': 78, 'B': 79}\n",
      "{0: 'k', 1: '8', 2: 'Q', 3: 's', 4: 'M', 5: '@', 6: 'G', 7: '!', 8: 'D', 9: 'S', 10: 'W', 11: 'N', 12: 'ç', 13: 'i', 14: \"'\", 15: 'H', 16: '$', 17: 'I', 18: 'K', 19: 'h', 20: 'b', 21: 'w', 22: ':', 23: '0', 24: '%', 25: 'X', 26: 'x', 27: '/', 28: '1', 29: 'J', 30: '2', 31: '7', 32: '9', 33: 'o', 34: 'R', 35: 'P', 36: ')', 37: 'a', 38: 'c', 39: 'u', 40: 'r', 41: 'd', 42: ' ', 43: 'm', 44: '\"', 45: '4', 46: '3', 47: ',', 48: 'C', 49: 'n', 50: 'f', 51: 'p', 52: '\\n', 53: ';', 54: 'Y', 55: 'v', 56: 'A', 57: 'F', 58: 'g', 59: 't', 60: '.', 61: 'V', 62: '5', 63: 'q', 64: '-', 65: 'E', 66: 'U', 67: '6', 68: 'y', 69: 'T', 70: '(', 71: 'z', 72: 'O', 73: 'l', 74: '?', 75: 'e', 76: '*', 77: 'L', 78: 'j', 79: 'B'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allosw us to create a vector of size 61 instead of 256.  \n",
    "Here and exemple of the char 'a'  \n",
    "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "The __loss__ is a key concept in all neural networks training. \n",
    "It is a value that describe how good is our model.  \n",
    "The smaller the loss, the better our model is.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the training phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculates the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the training set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculate the backward pass to calculate the gradients \n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function outputs:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "      inputs,targets are both list of integers.                                                                                                                                                   \n",
    "      hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "      returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "    \"\"\"\n",
    "    #store our inputs, hidden states, outputs, and probability values\n",
    "    xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    \n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  \n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    #init loss as 0\n",
    "    loss = 0\n",
    "    # forward pass                                                                                                                                                                              \n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "        xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "    # backward pass: compute gradients going backwards    \n",
    "    #initalize vectors for gradient values for each set of weights \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        #output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "        #derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y  \n",
    "        #compute output gradient -  output times hidden states transpose\n",
    "        #When we apply the transpose weight matrix,  \n",
    "        #we can think intuitively of this as moving the error backward\n",
    "        #through the network, giving us some sort of measure of the error \n",
    "        #at the output of the lth layer. \n",
    "        #output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " kItn7pqaKç*r,R.XN7jY:Gc47:qRz6vjDCVfwjvJH.F çPoCYg'8py7saS6O/P*9i85QiAA%udvsaR*ob@'bE%g%1pc.mIAx!/KXMF/dlw$V*?; @\n",
      "AhnGqE2gDiolB(wk73mt(4uJKJKi1nU8cç:8.Rds-J%hAzxd*S-5)-p:Kv6LTg\"302zB8C8nAsErzon!;\n",
      "e$6! \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    #for as many characters as we want to generate\n",
    "    for t in range(n):\n",
    "        #a hidden state at a given time step is a function \n",
    "        #of the input at the same time step modified by a weight matrix \n",
    "        #added to the hidden state of the previous time step \n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #pick one with the highest probability \n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        #create a vector\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "This last part of the code is the main trainning loop:\n",
    "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradiens\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradien technique Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [72, 49, 75, 42, 43, 33, 40, 49, 13, 49, 58, 47, 42, 21, 19, 75, 49, 42, 6, 40, 75, 58, 33, 40, 42]\n",
      "targets [49, 75, 42, 43, 33, 40, 49, 13, 49, 58, 47, 42, 21, 19, 75, 49, 42, 6, 40, 75, 58, 33, 40, 42, 9]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "This is a type of gradient descent strategy\n",
    "\n",
    "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "step size = learning rate\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.550677\n",
      "----\n",
      " e\n",
      "Ih6up4GTqX7dp:;O%1)md$US*oJNvKx'P u4D9 I/U9@0r7!9\n",
      "inmGVLc6qgW)@EIR0IaeH6'/uMsRm.CG OGNvElM)W3IzklwçOPfKh\n",
      "A5?VyxrEk$:BF?D,d'WaoJEEJçx\"2pBWW!hJVEdf6i2i1çW:dm8lwoHH\"rb8Nf5Ov7n)/wo\"*.FK4Ab yA%pnqWPk2?q* \n",
      "----\n",
      "iter 1000, loss: 85.476662\n",
      "----\n",
      " t soby Whest alaned on te ote blid hadis avealte visd aan the chese he 1rad hein y themyerd cdocanre r cth fa th to he.t l af tott, ih. fhag anlo teal \"er afge\"\"S nuth acithhe\",einkem anre te, - w waH \n",
      "----\n",
      "iter 2000, loss: 69.031388\n",
      "----\n",
      "  t, hirer hinmnisty hed her che In band sary mer hey, thut she mimg,rhoN.eA rulcaspinluntithenltudedeme, siathe aherermgod, ings hocon Nht se als ttheed hedinunhildhh, ise the fherien nofasel;y bomog  \n",
      "----\n",
      "iter 3000, loss: 60.910285\n",
      "----\n",
      " ly dald Grous, and sras hatt ilechid qomsof ete. Tfahld hitfrelyor,e was was wamopent ouger whecit hhes antl- ghad d wanp, thitn womEtheninpediltlmedhom,, anwis Gun he ais. \n",
      "t Stouls aldind.y, perasin \n",
      "----\n",
      "iter 4000, loss: 57.007069\n",
      "----\n",
      "  Grayg he he balvored hols the soub herinlsin ho?. Nutbim waon ank medegt this s maag atle ha ham aim the natAryen thy wom hoc alernin, tiat tor wasg to paepimet heund rove beeX, nis rathe cororst and \n",
      "----\n",
      "iter 5000, loss: 59.140520\n",
      "----\n",
      " e \n",
      "aadaith mor  oute orovenon heneuterltrouthe fenn yriveyr Prometete angh caorkacatevirg tuuin khiuf the mitortlittecut tousg thes vis worve d of wiell wosistin thy luy bafuto co (mact s fuso wo Go q \n",
      "----\n",
      "iter 6000, loss: 60.373703\n",
      "----\n",
      " parep, and rotliy sore the peand shad ofmed he thaed horg ther. Donk oorhe frimeingtrong is therbymeg the wooct hiot tay he bed he ipe h tork oneact ao the hiwslsouglouldvy wow argrhethattmentells fod \n",
      "----\n",
      "iter 7000, loss: 56.272377\n",
      "----\n",
      " ewe f-ars ttichr ad art the en hag kay Groudi g ime horg fre unrgand ong the wad to forple f that; stiste umunr in il son tas to burke relleluciltististoon wat is foof, not repat andlPe shis ard and t \n",
      "----\n",
      "iter 8000, loss: 53.264829\n",
      "----\n",
      " rt, aped unmed loals was bicos as in indot at enn tor fath at burt sheut bell chebe nted cain thever bout soser bea tomurytiall beat srun sGrearling the mak. Buteowen at tul. Greghiy Grepext asrhind s \n",
      "----\n",
      "iter 9000, loss: 51.870747\n",
      "----\n",
      " or wis folmislever fhery snounr Gracen ho eso chelein ol, ald buhind beyreive wixter on ono leacl has of the hingatirtimegrotere. Thand. The deter the noomnaded it indrads hat then corkid, he wowe lag \n",
      "----\n",
      "iter 10000, loss: 51.313211\n",
      "----\n",
      "  co lutd cotk lady woov allten not iod. Nom. fam tor poder, he thmed the hed heahe, coltor havek hror't\" in this as wamas jus was woundye med ste, boute fo cofl\n",
      "his tered wamd rowel ainn ok, unded the \n",
      "----\n",
      "iter 11000, loss: 58.593858\n",
      "----\n",
      " \"gute fosty mowimebatanI day ad theatay lod senyedancing wan tir Pthics ands lurtareng nott the Gander ald vaGs Maly IU codilp busing stegordi arricices asm. Grego Pot ag-y st tedyon staatidiped yojuP \n",
      "----\n",
      "iter 12000, loss: 55.165232\n",
      "----\n",
      " corcat, to wand he to the they nfel ram or the shaccrrieng hanly arstton inw Grefiigh sist, ther of weor ger evestis's yicdyry could hat s in mone acang alleb hastor dig, wat the chert?, Prelsrever th \n",
      "----\n",
      "iter 13000, loss: 51.825912\n",
      "----\n",
      " ooundidras that ceatever egor the awe, heisar:- niy sale pis ta was cholls thon.4 nomistouor's. The ghith\"\n",
      "and on nopharrcaplente'c wat hish anctate moo was ftimme on. pideley nomeremeed wated sat ept \n",
      "----\n",
      "iter 14000, loss: 49.880928\n",
      "----\n",
      " Grogher thas tor thand ptad tonlf beng he the backreuly aln thougher Gregor nothin the anong tly mace thas as. He thood, mattor's suskeve ard apre.\n",
      "\n",
      "\"Iqus muchen ald to soded'd sirluith ad hed bish to \n",
      "----\n",
      "iter 15000, loss: 49.127564\n",
      "----\n",
      " bath mowt unvy tore buclgapelan evening hil so toar alfrep as oaimses his in hack mute \"eous was and rao her to fade beaviye wheall an sils and thore ind alsor suwhim!C to the had lothe fo ba dimesten \n",
      "----\n",
      "iter 16000, loss: 52.410488\n",
      "----\n",
      " ou disorser bumnss wovion- the Fo asiinger frat stored bed Sout ow to the wougtid. He ther boj tha k-d it fo kithhhstendid bad and this be, whither'd, Pras, peracl plinclebnit. Sbrerofing lotenweurm.  \n",
      "----\n",
      "iter 17000, loss: 54.573819\n",
      "----\n",
      "  tos dide he five af the thand einincave loXl ple th hrat at. Noo Dacl be, siourden m91after Andsting breghenchan, ow his evente lovergted hr aind hor but. dey uthit tos to got tome on inal the got ho \n",
      "----\n",
      "iter 18000, loss: 51.607479\n",
      "----\n",
      " ome wimher sher at het leated of as fis to ien thand cor en nouk cafalp to ttoon oversedly in sie in waLlldy, spel nuthion mem at fate ald bearsibus and to hrave would and nalach\n",
      "eper the ettimping th \n",
      "----\n",
      "iter 19000, loss: 49.196380\n",
      "----\n",
      " Simhe fire\"ne Ig for not boousm nothinka potis hilpenlesed ordthand's his llipy Ot as wat undetfad gelli eacmurg the wous the sisted so treaf that becater the stither withhe ansed it eved. Grangoull a \n",
      "----\n",
      "iter 20000, loss: 48.299540\n",
      "----\n",
      " atarsen pangar and, hap wever noumd chit gf whald laid go hiby thtrewing dion opanner toke that so had sere bat. preghe and. Hf int achilrose wer of hale, in ulrred tist him to hamud sioned of be the  \n",
      "----\n",
      "iter 21000, loss: 48.093146\n",
      "----\n",
      " s livering, mitsedin! Ath alfol hack -tinct chrealle for munharf reat lain, aten ded in juch in her, lfafh simlithe has oft. But uteraso anf en that the sfor whish whimed whalkeam thit, t oull lats st \n",
      "----\n",
      "iter 22000, loss: 54.718491\n",
      "----\n",
      " kered thay a done was wtre der wiorar (R 1..\n",
      "\n",
      "; arrect, evillit oned wtoke wide adreno k0P0Iy, -lt/re ush\n",
      "pectov. M/ound lrocitht\n",
      "cojor anasif tatiovroncannised Grmas goruby itatp, woudrnistiob the P5 \n",
      "----\n",
      "iter 23000, loss: 51.882075\n",
      "----\n",
      " ende nath say had midsiver tor the could. Anderszic meether of mrace fire he bea dreceanly, her a ton; thare be. fr themeng thead ther clicbut and hise woosenst be beadisfingin lith axk diy felg the b \n",
      "----\n",
      "iter 24000, loss: 48.983822\n",
      "----\n",
      " ctreser weate nlock trachald woursounf ely and leved nocht hand dicte andonce emithen the hing sidrimss't when be puaft hlre chet Grem. It what that Greensid. \"Whe ifkmy parserse forle beebute the dac \n",
      "----\n",
      "iter 25000, loss: 47.271413\n",
      "----\n",
      "  st, havill ded anof shanr on to hes allly. Than ry, thomen to hawe Douscen, ttool itter sitece pare he wough stillwing dad thougher; though ifwe wwould weoong vor it could daly the of the way ray peo \n",
      "----\n",
      "iter 26000, loss: 46.874276\n",
      "----\n",
      " bomeg of the would whid anven.\n",
      "\n",
      "And it hm mand he tor forly his blye to ho puld hor't mowly tural the door the livencaned t mometery, lrong, on porinm his nover on hes Greging lo me the ferech on, nea \n",
      "----\n",
      "iter 27000, loss: 49.875049\n",
      "----\n",
      " w\n",
      "wans anssst steng mo and dof? She cheyen tist oT. Ar cthelr\"Foon no. Greester\n",
      "5.\n",
      "\n",
      "1.E\n",
      "1.EK whem prefoly lengssed the diowed of they ereked. wire stecel degat of ortedebs abount whim exsting and, sti \n",
      "----\n",
      "iter 28000, loss: 51.901908\n",
      "----\n",
      "  Ther Ss miomserm theiseld bey on the oforthulcozing in the ar ecu fow thithw dif have. Greformed he sily nuseng the cay tute though dosbe fain or wat peaf that wacfor one? II  to marepliinctibe whive \n",
      "----\n",
      "iter 29000, loss: 49.190035\n",
      "----\n",
      " beersirt3 thate sike itt ing, wike Pus onked and the mothodd quen mo jegol hall in yther? I've beg of mpeve bronly thou't op coack to on sher't mone thinkching the ad out farst the cork noo goth has w \n",
      "----\n",
      "iter 30000, loss: 47.105235\n",
      "----\n",
      " en thers wee fio cheingraty kevionying hit er with, of muthingercay mong he wass begor curd ampester ing ifm, whated wing ara dacley fas of hadow Grom had as rag ked that aging a oL, hid theme  he hes \n",
      "----\n",
      "iter 31000, loss: 46.415093\n",
      "----\n",
      " pentourly at, ifdew tik the ked. And call Gregor brouth hed wat hase as t:re Greviny. On then theis hall thing they and ofle to he they, lesken agavayoen in an git - she etise, on thind h ars and the  \n",
      "----\n",
      "iter 32000, loss: 46.278040\n",
      "----\n",
      " owe it of the ot the wist nound bijuse to the room bucheaso stoout he Fow's when it's limstreit buynk thep heed whand cany baked fol he har he he mast bloth, pureste wat thcon urcinsspitwating the doo \n",
      "----\n",
      "iter 33000, loss: 52.173132\n",
      "----\n",
      "  Lisar\n",
      "- Learnit kefte, that the mict paly of njis pors that Wercest io sich pusend uran a woring ullecti ipvess it and he jo Project's frreates sal og redoonays\n",
      "it the raite to nusenbusannintrojenp m \n",
      "----\n",
      "iter 34000, loss: 49.880723\n",
      "----\n",
      " l you her peapl beanxve, Prow, momed wise ceelf fiolsing, a futhery. An quiectid ne!\") was the fistise paring dore siuse netibly he ptort hat lithre be't meno coje to penesingoong androjed oquse ht\",u \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 47.309255\n",
      "----\n",
      " h comyer youid but, in nomerome in. Guldeaner liat his him tore Grogatmaly flor as stow her ntint fiing wwaded poce vere for proktrothawing he to lo6ginge yim ans anere ther - sunt called, hing mway o \n",
      "----\n",
      "iter 36000, loss: 45.720965\n",
      "----\n",
      " , hound hail her exthe they would sien-t had it pive fhin oud stanamey insely en heright had ad nooon fortsine with in came alr and, as laked to dot in in hes fann had, wands as t af the wore fathen t \n",
      "----\n",
      "iter 37000, loss: 45.450269\n",
      "----\n",
      " avrarmit olald armpeing stor.\n",
      "\n",
      "p. Gregor fuchen mattat nooms mike in apintily may puopaed to nechs, hid cemening a tork suplo-d it. Niat fotime rols antid. He regetherwadkurest fothun he brame, furnid \n",
      "----\n",
      "iter 38000, loss: 48.087924\n",
      "----\n",
      " uecof losm.g even not vikeng becrelensing it arxw the plestard of he lot it pitaverdinend shis hing - fakem to apliound with riwhey to inkow thool he it cea c1 Grecuted outen\" work and so thane to lit \n",
      "----\n",
      "iter 39000, loss: 50.243159\n",
      "----\n",
      " daze to the atGreger herp to nooo fal of the drouteared from vesivs coupdore woute Yend lemsincact his have sive tullic the wheles clemaly her you there got it Rven So to hackiber It would that uin co \n",
      "----\n",
      "iter 40000, loss: 47.763389\n",
      "----\n",
      " ist ap ingumartis hald a fle thith secely Gregether, chaiemibuld wouldest it woven. \"He and. He gethed on bentlat, as has, with whimings ibam yous west she mivand deral. wio hey could leineng stor.\n",
      "\n",
      "\n",
      " \n",
      "----\n",
      "iter 41000, loss: 45.819710\n",
      "----\n",
      " mes bo-thoug hil serisk and oo. Fuccuid the ered, pee swere thut she 3hough bey, hole nothele in his the chit letseme, \"*ong oughing to lister hes onftrad wand arach that in hes fregureghaly allf adse \n",
      "----\n",
      "iter 42000, loss: 45.263396\n",
      "----\n",
      " eadrasf he and eround surmysese tixted. He ras tem worker yifulder the \"Ior the Grego add into be the fablale and dost wat achit rean wist her if then tlwimlins tomem allemy the dicferme beforp tory,  \n",
      "----\n",
      "iter 43000, loss: 45.161821\n",
      "----\n",
      " msing mother peard to glomel all to dory fortos, him thear of thee vers he hind was ally he use Gute beand pullisusien then, it the eanmsoont unde her wastend the peofet ettoucpoor; tome farist inthis \n",
      "----\n",
      "iter 44000, loss: 50.558308\n",
      "----\n",
      " orliontuly projedt it enitw dedef react tlwiat lay, Pres ine motpices hat or and to pliess bfors worlertliger. Se fastar ofted threes intion and ow on for 8ettorsep in to norbe.\"  oN wat ony bith thou \n",
      "----\n",
      "iter 45000, loss: 48.602711\n",
      "----\n",
      " u hapave ster 4ank a's entaing th thes ist reined him goss ino giesl baskn. Mrvlat. Bu his ppoon.\n",
      "\n",
      "Sough courditir to reisecy had fare tunchtithougpen whit and parelnte doon work dee, spew of, stoughl \n",
      "----\n",
      "iter 46000, loss: 46.214446\n",
      "----\n",
      " in ne agantly veaslay lebenibe he llemny peen and was's alceale to nook) and weat's mo kas tally into his fothing sercpean ean semt futhoughar asfort ap coutk anf har wast the muntax thit comantcoustl \n",
      "----\n",
      "iter 47000, loss: 44.708666\n",
      "----\n",
      " sad had in brest had tole wout ulthay to, wails to was has shalnted bis and theer and abrechis, a fioor't aref age feact oboursid in it nor chy unpsether out tisted disp all Aplo,, come for, co meyige \n",
      "----\n",
      "iter 48000, loss: 44.452948\n",
      "----\n",
      " n. Tay the red of tice noa him bourd, cain't becouthly fusher the but lobud beadarntwarvly but ha waiven would ray time verchere would now and rousoor mey thi dather dig to her inter, a wher'ed aqule  \n",
      "----\n",
      "iter 49000, loss: 46.838813\n",
      "----\n",
      " co daacl dayid the dor wond wio romery thong wat timy. The for't hs payiste pice sprither cewapyed - and untarfarmidsle lige antoom that oom Snomher and the chate hes so be, the frem onitirg-ted in in \n",
      "----\n",
      "iter 50000, loss: 49.095431\n",
      "----\n",
      " all far, quetrote werme in and to forly the nong bound hind ligelin, bregor fallelt she bout wat sarpenty aftamiles epest and saver and t hist hem his wookerin't ofre prought uplled the her nethe hark \n",
      "----\n",
      "iter 51000, loss: 46.791523\n",
      "----\n",
      "  backen breed havim omsistlend thive her his to from bomered at. Youce cleed the was a\" ifusing, kion. whew fork of molf then thy deel leairse, red Bight frountiet, be sorar home and, was to drabsoder \n",
      "----\n",
      "iter 52000, loss: 44.932775\n",
      "----\n",
      " l the war sort hroulf of the sto her tist, the pormedef tho Grat houch at had to raise to and, sionel, note sars., gort, his way fhere, his acture not in the hay, they ever wort of tist on. Gregow, mo \n",
      "----\n",
      "iter 53000, loss: 44.469847\n",
      "----\n",
      " xer foulf a torson peit. Some. So monet the coming of re sard in bare t were dooren his for ann, to phrars braine say he aboutle whylete sostint wire ummy; or therury, the falle mading. Gregor. There  \n",
      "----\n",
      "iter 54000, loss: 44.347914\n",
      "----\n",
      "  whthing the oftily and the phed tham, so the qued for he af hereand the\n",
      " quce moven this sided has ArGus hew the lim. Gregor's comanmantrow. \"He they extiusbely of the it, shat pllred her't at suad n \n",
      "----\n",
      "iter 55000, loss: 49.301228\n",
      "----\n",
      " ant/punnnitesttarges, out atmaace on a's a  retss.  on poringea sid stine the renfroic-tw the batit, at.\n",
      "\n",
      "1.E.1.  Prayspides am tork\n",
      "CImed\n",
      "\n",
      "wat if ceat intourtimy so trasebe wirgorntrigerce.  That mov \n",
      "----\n",
      "iter 56000, loss: 47.663077\n",
      "----\n",
      "  of so t at in tlo futet Gregriect's bus of with he sion the boor. Lighaw. It therk JaCfernity to hery wost beand bawing and thas mor perpicier then his hand ay be surs of this, menly Anor't Prey had  \n",
      "----\n",
      "iter 57000, loss: 45.376284\n",
      "----\n",
      " laade hee an was wand aw?\"\n",
      "Ir they rowe did out emed you'd Gretherd sios harthool, ort shaill he give dice for him the uhe they hads at not siturithom\n",
      "\"regh pregor't abe thee of his musher the lilk, i \n",
      "----\n",
      "iter 58000, loss: 43.982019\n",
      "----\n",
      " rnteng kis bowly if fered, the payen the cherazed to but dides, and room chis could beluse tovew starecad, brecimey or, but, mor hau bigh, whing and a and hem benly hed mame them his evintty he pros i \n",
      "----\n",
      "iter 59000, loss: 43.709776\n",
      "----\n",
      " ence mated inw, a been onouslur, yougestiull amefulsiding wat wlly and oprentroughe ofo hor his  use takad lodked lock alled hay soul so he sient and tary to he cond ford everyed shail to surecht tull \n",
      "----\n",
      "iter 60000, loss: 45.917460\n",
      "----\n",
      " ve to geracw wiisisormat withaly phe alrention. OR It the gor on sallory the cowl- enbute beviarcerag fros sad mander nigd 9us if and hr soming oose meded, spry anr fimhily llend Gr aga ded and bit. K \n",
      "----\n",
      "iter 61000, loss: 48.209541\n",
      "----\n",
      "  the fable.\"r ut the uld ite ster the eadaly hungsimit forquect freal far on to oft to reyswith he been but Gregor\", Projpcould the feeple stactuine vrowned ht fichs oxping simivilish\" (or undenall's  \n",
      "----\n",
      "iter 62000, loss: 46.052728\n",
      "----\n",
      " e anaring the chide, his't the spatemigharn acw all ml) stowand him; theurd in thine that sh aman the now umanted of loke ons, mover'e coslly his glest voss at thm was and of ane lot? She kist coucht  \n",
      "----\n",
      "iter 63000, loss: 44.230944\n",
      "----\n",
      " ovelly to foom of it, beem allo justuther fachle fain ash hid the on; seave his and have foswer't becat with has moth it sighar he gere eft ard haed of to this locer ermesstithing, screarall.\n",
      "\n",
      "I have' \n",
      "----\n",
      "iter 64000, loss: 43.828723\n",
      "----\n",
      " was sate the to his the ew as nf he sain theime that it inst 3o would hem so weate ankugore cook ther blias himsechinburanatenk beellle. And now th the suh dos by her sight ciking it not hereathed had \n",
      "----\n",
      "iter 65000, loss: 43.703840\n",
      "----\n",
      "  hime makiduy. What his sher thers could wnan the casan woull she wand to and ottooks, dechaise in the doob wage whied she alstesting aby?d, misting his and rrom staneod ar\" pantterd, courgat. Ather,  \n",
      "----\n",
      "iter 66000, loss: 48.305719\n",
      "----\n",
      " it domxor his seatolfing on. Gut of's fore, an\n",
      "would to it lagmie, uf par (inceryedtoy expectrigl.  Gr jequiotw./\n",
      "T fromime at fort the dady dod, to domigulartule a the egatis fuch rigented\n",
      "\n",
      "all aOf P \n",
      "----\n",
      "iter 67000, loss: 46.927685\n",
      "----\n",
      " happerted his mut get int has renuthe to and raivemmaiting bidistl-tines hespe igord it coin thatigr now arreer, eno sh Gregor quin the frow sif the mamuised work And parrzinghenms the mad. - he nle w \n",
      "----\n",
      "iter 68000, loss: 44.729899\n",
      "----\n",
      " an bythe seleght wandere of not of should beally, her. Though eating intood had time carm hack fus red the chimamacher on win was allo cre0ent and has. Preanted, vright rogaigh sarar in ho in he shuim \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 43.399241\n",
      "----\n",
      " arpealding if he  it theme goting his balle boctory had his her then's Unother wewlors \"iching disseen Gregurmiceave iove sush and to ef had it anth\n",
      "ad cery twis, a terither's ef h's un the satiste, t \n",
      "----\n",
      "iter 70000, loss: 43.089134\n",
      "----\n",
      " abourk benent; itics onmion to spany boke? The toud for lien was us, her if he so afriel on to stoons the mather's his wase clead bactle, on hearfone at to wfeed the storks. Wapand, to the anaurf to m \n",
      "----\n",
      "iter 71000, loss: 45.265598\n",
      "----\n",
      " ors motertung the wood trod, domenf ana\n",
      "trome beed the doucd it whele the arersy and whey Grisesce in of the cformaturst the saveprisge complest in thitt, prestadpectrof anly, at ont, fomel. Fowin boo \n",
      "----\n",
      "iter 72000, loss: 47.487705\n",
      "----\n",
      " ly ans ull the tore cout mak olly thong the had his hanmed nuil 3lsion it courdise fort hew thind, ers and has furk that posiek or cemplealy of up torges all ly, ait blenief ris it, of she wit was thi \n",
      "----\n",
      "iter 73000, loss: 45.422554\n",
      "----\n",
      " n the halb-ly he gent to should my to\n",
      "meslefulf they doort Ilplintly, anferriveng. He exwore clith that hed and awt usare his rof ie was from the bearing, all rey emlain the fither the kforks, room no \n",
      "----\n",
      "iter 74000, loss: 43.665013\n",
      "----\n",
      " now hards to mother't to chail, the room, ho felled from paresh the a turely havembing and long to nots and was beenut onintly now the Umew it yestesint, wool\n",
      "Ciseno dist. Pave the conat mome there cu \n",
      "----\n",
      "iter 75000, loss: 43.331642\n",
      "----\n",
      " istle's plews himsento 4fack tome hay anplithes thing abless felat ots of the way the chey waurd eardbel of nuit Gregoo fiolar upled coopion, that himpaured veryed alice could ce, wall hing oxply the  \n",
      "----\n",
      "iter 76000, loss: 43.162808\n",
      "----\n",
      " sisterd ippenting; it; it. That tharrd of her had notelo brearlysmed lecarused ow\".\n",
      "\n",
      "1.dr, and buged to hin. He he sfort winllatliped.\n",
      "\n",
      "The baurstincorclode now no Sigl0yopisal toring had helt he hed, \n",
      "----\n",
      "iter 77000, loss: 47.459188\n",
      "----\n",
      " be thet Gunte orfeised to with frais to batt over apeded allenbatrogh worling, and vout ofteriffoll on Preave thad matfieccown (oo more pacone bat ttes\n",
      ", Pregor expleartenf ince aid at a wither ytersi \n",
      "----\n",
      "iter 78000, loss: 46.369543\n",
      "----\n",
      "  drest woull has amp-tevrequing he limpelk lipker the  hay the not would and pempforchings it thisgor't man tway stor noted in tim to hhe and ave semurgat of clomeine sexwy had whele, all he ffies and \n",
      "----\n",
      "iter 79000, loss: 44.196740\n",
      "----\n",
      " e seemauty, beclest sigsintisite siom, omsast dist, befepligetolk the roat eaed ropped op hasid inw. Himge had fut he's ferev was or's nonts,, cregor ereant fer at from any obuch wald learagoop oK the \n",
      "----\n",
      "iter 80000, loss: 42.928596\n",
      "----\n",
      " ot's sat father cayed tudeced ofed, a should him sust the way and of the doore whthe was racher a. mound was not the shl's  he her he  and ave to span and he at intakise refo dadakings ontood eple. Dh \n",
      "----\n",
      "iter 81000, loss: 42.644705\n",
      "----\n",
      " , of upshe fane deretigter abothel he mive there mait takes bos in; her voored ttiaspen\", was lootention ce, lorken can tingshirg. Gropticg as couve iforcottad to seant to the ofttes he cle sse a meed \n",
      "----\n",
      "iter 82000, loss: 44.684436\n",
      "----\n",
      "  straen bealcested sua reaned icleghing urlectingine, andispics.  ammeyod; gran did ble my that that and stact ofresmistion wethor way; the grom touse she more not's of nother sistrieg the wackis mari \n",
      "----\n",
      "iter 83000, loss: 46.886965\n",
      "----\n",
      " ith worked her becentworis as mo forbing the chist sares hes ersen-teng Gregor Was 4emainf. defll a sataor untertht, Loons\n",
      "mow him cowt by and indieic fainted contiug\"Liousio\" had wamseed thater unso  \n",
      "----\n",
      "iter 84000, loss: 44.980635\n",
      "----\n",
      " enl hhe etted whonly in she feref not and slemsed ssazing had\n",
      " whonil whinstown.\n",
      ". Yon tike wheret coite. The a of thould hice shile he his he would bel0.\n",
      "\n",
      "\"Whe was of homartsiftly. Ot onbuls cloke br \n",
      "----\n",
      "iter 85000, loss: 43.210649\n",
      "----\n",
      " hey swosting with beced hmore to me-ed and that they how striigtime in whene then could not besimperstressmenenbous a asce avanlict of you godisan more the emally his fashur anrsated. On't pelicd and  \n",
      "----\n",
      "iter 86000, loss: 42.951416\n",
      "----\n",
      " s a ray emide, camasing and with he soeersed at mister he food romels himser.y Gregor word mestilisile his sean as at had, under.\n",
      "\n",
      "1.J semeneayed of thot, himhen, fant it an he. He saol, own marefnect \n",
      "----\n",
      "iter 87000, loss: 42.719271\n",
      "----\n",
      " ere. Mle meess as at not could free of her that's not has gerwing dreatwirsed the doorew rost it lowk doom her as if the asoly clooth dra; well bore it oftiled alling, caboun wo doreff to mis his had  \n",
      "----\n",
      "iter 88000, loss: 46.762797\n",
      "----\n",
      " can by or be't, Projectr work claris formistoin, bristlapsed of thind\n",
      "co intertibereds proostrstation aropewt oBm to mevercoulding, Grogrly fior cettrary couse woul?  Ye unet come of a clentowt co@ref \n",
      "----\n",
      "iter 89000, loss: 45.895196\n",
      "----\n",
      "  the firk tevery.\n",
      "Tor han patite as and in yeben was a as this tontes whane comnigatropring to it to \"Mist to mall was three of the tomering oler each up cowleng it all wither to snornenttrart. \"The w \n",
      "----\n",
      "iter 90000, loss: 43.743093\n",
      "----\n",
      " lefning thoumlitore\", pachrel selis, and he mothey cemping.\n",
      ",using alppend his crink. It aid and more Gregor say of hin the his lot arrain I stat, way, would hid rose would, as ans wast by thel at and \n",
      "----\n",
      "iter 91000, loss: 42.609204\n",
      "----\n",
      " aticy for chicksing, staim-ne so the sis alysiys that filftar soon, low to eack of his has with promen of Vall bghilicate wither so cother get his kust and hod her. What want and he litted the paiclo  \n",
      "----\n",
      "iter 92000, loss: 42.250916\n",
      "----\n",
      " nt and emeding the iftave the lealpring sick, them qulmsithtsong to was fath, in then could avieghat gore did, viugh 1novackint featroon, ed in eace his cound: This ef he uses der to hairedons prow th \n",
      "----\n",
      "iter 93000, loss: 44.195793\n",
      "----\n",
      " odenf the Fould work raaremed, desithy whit he wish Pray yatenyicit and not to ied a lacoupentros what allay, begrood disesory and the  theid the's Gregh:'d ofely if work Gregoried the woyly a sionrit \n",
      "----\n",
      "iter 94000, loss: 46.403966\n",
      "----\n",
      " this egation day he Sack it to bus so dot faider he ifome the fers. Had to domane by not hive tofred to und.\"\n",
      "\n",
      "Them  the fell\n",
      "peegy, wory sut the camessor in whief. She leke watistelation of coplly lu \n",
      "----\n",
      "iter 95000, loss: 44.573167\n",
      "----\n",
      " gh mosely my to the viung the tore abreano out the der all youg arready. Nush all wird stort-ter the couyen urefer Gregor and nort the of urego if the trountiuty, contlach, of the charalt of chrarl in \n",
      "----\n",
      "iter 96000, loss: 42.846734\n",
      "----\n",
      " nunning nush to fetwes as he theiered for wheal had his; his to medig. Phough, crough now a him eave thing they so the with, dave theired had his dooult that hish whearping, wopered sadectoar shat eve \n",
      "----\n",
      "iter 97000, loss: 42.640677\n",
      "----\n",
      " ountrooc begall Led. Grepweng joom it waye hos ley and beveren the woad was a simtecufoon thouch anations but firs ave she what thar mpreflide  whinf tupening slewout he would his fattoves weare her a \n",
      "----\n",
      "iter 98000, loss: 42.394142\n",
      "----\n",
      " should back ing.\n",
      "\n",
      "\"He, and as havance been. \"M! Fhe room \"Gregorratsed him notlowl And his leynation inftor and hu says hes fit, be.\n",
      "\n",
      "Whey thuts soms had beg the bernats to could or in med. Grrait, an \n",
      "----\n",
      "iter 99000, loss: 46.194550\n",
      "----\n",
      " e, siotion oomall\n",
      "with, on, insod\n",
      "\").An at oetine, yout wirm wire the belend work withome com shist ovreflurses to and sistoin.  That the Flipentor't Gregorce it what this hest been thout decewing ta  \n",
      "----\n",
      "iter 100000, loss: 45.435975\n",
      "----\n",
      " ch, and with of all has  pood with thile ichall cear. If the ally and plewhains therr sovible sapepor; the dise riffor to was dous to on his leyed \"I mode ked and, ale lut \"ancamangst ine hap by but t \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6aad694e7d34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6f0183708426>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# encode in 1-of-k representation (we place a 0 vector as the t-th input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Inside that t-th input we use the integer in \"inputs\" list to  set the correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=10000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    \n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "    \n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
